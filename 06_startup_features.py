# -*- coding: utf-8 -*-
"""06_startup_features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PE_PKHpsPQB5U2qnzjELGEV6JIhwFG6R
"""

# =========================================
# Startup Risk Analyzer
# 06_startup_features.ipynb
# Cell 1: Load Cleaned Data
# =========================================

import pandas as pd

# Load cleaned startup dataset
startup_df = pd.read_csv("data/cleaned/startup_risk_cleaned.csv")

print("Dataset Shape:", startup_df.shape)
print("\nColumns:")
print(startup_df.columns.tolist())

print("\nSample Data:")
startup_df.head()

# =========================================
# Cell 2: Create Startup Risk Labels
# =========================================

# Create funding-based risk labels using quantiles
funding_quantiles = startup_df["amount_usd_clean"].quantile([0.33, 0.66]).values

def assign_risk(amount):
    if amount <= funding_quantiles[0]:
        return "High Risk"
    elif amount <= funding_quantiles[1]:
        return "Medium Risk"
    else:
        return "Low Risk"

startup_df["risk_level"] = startup_df["amount_usd_clean"].apply(assign_risk)

print("Funding Quantiles (USD):", funding_quantiles)

print("\nRisk Level Distribution:")
print(startup_df["risk_level"].value_counts())

# =========================================
# Cell 3: Feature Engineering
# =========================================

import numpy as np

# Log transform funding (reduce skew)
startup_df["log_funding"] = np.log1p(startup_df["amount_usd_clean"])

# Reduce category cardinality
def reduce_categories(series, top_n=10):
    top_categories = series.value_counts().nlargest(top_n).index
    return series.apply(lambda x: x if x in top_categories else "other")

startup_df["industry_reduced"] = reduce_categories(startup_df["industry"], top_n=10)
startup_df["city_reduced"] = reduce_categories(startup_df["city"], top_n=10)

print("Top industries after reduction:")
print(startup_df["industry_reduced"].value_counts().head(12))

print("\nTop cities after reduction:")
print(startup_df["city_reduced"].value_counts().head(12))

# Final feature set
feature_cols = [
    "log_funding",
    "industry_reduced",
    "city_reduced",
    "investment_type_grouped"
]

X = startup_df[feature_cols]
y = startup_df["risk_level"]

print("\nFeature Matrix Shape:", X.shape)
print("Target Shape:", y.shape)

# =========================================
# Cell 4: Encoding + Train-Test Split
# =========================================

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer

# Separate categorical & numeric columns
categorical_cols = [
    "industry_reduced",
    "city_reduced",
    "investment_type_grouped"
]
numeric_cols = ["log_funding"]

# Column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols),
        ("num", "passthrough", numeric_cols)
    ]
)

# Encode target
risk_encoder = LabelEncoder()
y_enc = risk_encoder.fit_transform(y)

# Train-test split (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y_enc,
    test_size=0.2,
    random_state=42,
    stratify=y_enc
)

# Fit-transform
X_train_enc = preprocessor.fit_transform(X_train)
X_test_enc = preprocessor.transform(X_test)

print("Encoded X_train shape:", X_train_enc.shape)
print("Encoded X_test shape:", X_test_enc.shape)

print("\nTarget Encoding:")
print(dict(zip(risk_encoder.classes_, risk_encoder.transform(risk_encoder.classes_))))

# -----------------------------
# Save artifacts (NO MODEL)
# -----------------------------
import joblib
import os

os.makedirs("/content/data/processed", exist_ok=True)
os.makedirs("/content/models", exist_ok=True)

joblib.dump(X_train_enc, "data/processed/startup_X_train_enc.pkl")
joblib.dump(X_test_enc,  "data/processed/startup_X_test_enc.pkl")
joblib.dump(y_train,     "data/processed/startup_y_train.pkl")
joblib.dump(y_test,      "data/processed/startup_y_test.pkl")

joblib.dump(preprocessor, "models/startup_preprocessor.pkl")
joblib.dump(risk_encoder, "models/startup_risk_encoder.pkl")

print("\nâœ… Startup features & encoders saved successfully!")

